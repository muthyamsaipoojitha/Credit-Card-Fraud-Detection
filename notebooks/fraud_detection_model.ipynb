{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf24096-48e1-4a37-b457-041692339c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully with ISO-8859-1 encoding\n",
      "Columns in the dataset:\n",
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "Dataset Head:\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n",
    "\n",
    "# Step 1: Load the dataset from the specified path with a different encoding\n",
    "file_path = r'C:\\Users\\saira\\Downloads\\Credit-Card-Fraud-Detection\\data\\creditcard.csv'  # Update with the correct file path\n",
    "\n",
    "# Try loading the file with ISO-8859-1 encoding\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding='ISO-8859-1')  # Using ISO-8859-1 encoding\n",
    "    print(\"Dataset Loaded Successfully with ISO-8859-1 encoding\")\n",
    "except UnicodeDecodeError:\n",
    "    # If ISO-8859-1 doesn't work, try utf-16 encoding\n",
    "    data = pd.read_csv(file_path, encoding='utf-16')\n",
    "    print(\"Dataset Loaded Successfully with UTF-16 encoding\")\n",
    "\n",
    "# Step 2: Check the column names in the dataset to identify the correct target column\n",
    "print(\"Columns in the dataset:\")\n",
    "print(data.columns)  # Display the column names\n",
    "\n",
    "# Step 3: Print the first few rows to inspect the data and check for the target column\n",
    "print(\"Dataset Head:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 4: Update the target column based on the correct column name\n",
    "target_column = 'Class'  # 'Class' is assumed to be the target column for fraud detection (0: Non-fraud, 1: Fraud)\n",
    "\n",
    "# Step 5: Handle missing values if 'Class' is found\n",
    "if target_column in data.columns:\n",
    "    data[target_column] = data[target_column].fillna(data[target_column].mean())  # Replace missing fraud data with the mean value\n",
    "else:\n",
    "    print(f\"Column '{target_column}' not found in the dataset. Please check the column name.\")\n",
    "\n",
    "# Step 6: Drop non-numeric columns for feature engineering\n",
    "# Remove columns that are not useful for modeling like 'Time' or any other non-numeric columns if applicable\n",
    "data = data.drop(['Time'], axis=1)  # For example, if 'Time' is not useful\n",
    "\n",
    "# Step 7: Prepare the feature set (X) and target variable (y)\n",
    "X = data.drop(target_column, axis=1)  # Independent variables (all columns except 'Class')\n",
    "y = data[target_column]  # Target variable (fraud detection)\n",
    "\n",
    "# Step 8: Feature Scaling\n",
    "scaler = StandardScaler()  # Ensure StandardScaler is imported\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 9: Train-Test Split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 10: Model Selection and Training\n",
    "# Using Random Forest Classifier (you can use other models like Logistic Regression, XGBoost, etc.)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 11: Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Accuracy, Precision, Recall, F1-Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")\n",
    "\n",
    "# Step 12: Visualization (Confusion Matrix Heatmap)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Cross-validation (for model evaluation)\n",
    "cross_val_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cross_val_scores}\")\n",
    "print(f\"Mean cross-validation score: {cross_val_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda60a9-d0d7-4c72-bef1-57b3913035ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
